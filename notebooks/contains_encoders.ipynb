{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-15T18:46:54.320541Z",
     "start_time": "2022-10-15T18:46:52.145268Z"
    },
    "code_folding": [
     32,
     44,
     54,
     74,
     88
    ]
   },
   "outputs": [],
   "source": [
    "import pandas  as pd\n",
    "import numpy   as np\n",
    "np.random.seed(42)\n",
    "import seaborn as sns\n",
    "from matplotlib            import pyplot          as plt\n",
    "from IPython.display       import Image\n",
    "from IPython.core.display  import HTML\n",
    "from pylab                 import rcParams\n",
    "from IPython.display       import Image\n",
    "from scipy                 import stats           as ss\n",
    "from sklearn               import model_selection as ms            \n",
    "from sklearn               import preprocessing   as pp\n",
    "from sklearn               import ensemble        as en\n",
    "from sklearn               import neighbors       as nh\n",
    "from sklearn               import linear_model    as lm\n",
    "from sklearn.model_selection import KFold, cross_val_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics       import accuracy_score, f1_score,classification_report\n",
    "from sklearn               import neighbors       as nh\n",
    "from sklearn.ensemble      import RandomForestClassifier\n",
    "from category_encoders.leave_one_out import LeaveOneOutEncoder as leave\n",
    "from category_encoders.woe import WOEEncoder as woe\n",
    "from category_encoders       import TargetEncoder\n",
    "import xgboost as xgb\n",
    "import sys\n",
    "sys.path.append('/home/soturno/.pyenv/versions/3.10.4/envs/pa04/lib/python3.10/site-packages')\n",
    "import scikitplot          as skplt\n",
    "# pd.options.display.float_format = '{:,.2f}'.format\n",
    "import warnings;   warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# sns.set_style(\"darkgrid\")\n",
    "# plt.style.use(\"default\")\n",
    "# rcParams['figure.figsize'] = 14,5\n",
    "\n",
    "def precision_at_k(data,k=2000):   #2000 é apenas o default caso ngm passe o valor, poderia ser outro valor como 20000\n",
    "    # reset index\n",
    "    data=data.reset_index(drop=True) #necessário para q o index nao seja igual aos ids\n",
    "\n",
    "    # create ranking order\n",
    "    data['ranking']=data.index+1    #(para ñ começar do zero)\n",
    "\n",
    "    data['precision_at_k']=data['response'].cumsum()/data['ranking']\n",
    "    \n",
    "    return data.loc[k,'precision_at_k']\n",
    "\n",
    "\n",
    "def recall_at_k(data,k=2000):   #k poderia ser outro valor como 20000\n",
    "    # reset index\n",
    "    data=data.reset_index(drop=True) #necessário para q o index nao seja igual aos ids\n",
    "\n",
    "    # create ranking order\n",
    "    data['ranking']=data.index+1    #(para ñ começar do zero)\n",
    "\n",
    "    data['recall_at_k']=data['response'].cumsum() /data['response'].sum()\n",
    "    \n",
    "    return data.loc[k,'recall_at_k']\n",
    "def embed_features(learner, xs):\n",
    "  \"\"\"\n",
    "  learner: fastai Learner used to train the neural net\n",
    "  xs: DataFrame containing input variables. Categorical values are defined by their rank. \n",
    " ::return:: copy of `xs` with embeddings replacing each categorical variable\n",
    "  \"\"\"\n",
    "  xs = xs.copy()\n",
    "  for i,col in enumerate(learn.dls.cat_names):\n",
    "    \n",
    "    # get matrix containing each row's embedding vector\n",
    "    emb = learn.model.embeds[i]\n",
    "    emb_data = emb(tensor(xs[col], dtype=torch.int64))\n",
    "    emb_names = [f'{col}_{j}' for j in range(emb_data.shape[1])]\n",
    "    \n",
    "    # join the embedded category and drop the old feature column\n",
    "    feat_df = pd.DataFrame(data=emb_data, index=xs.index,               \n",
    "                           columns=emb_names)\n",
    "    xs = xs.drop(col, axis=1)\n",
    "    xs = xs.join(feat_df)\n",
    "  return xs\n",
    "def calc_smooth_mean(df, by, on, m):\n",
    "    # Compute the global mean\n",
    "    mean = df[on].mean()\n",
    "\n",
    "    # Compute the number of values and the mean of each group\n",
    "    agg = df.groupby(by)[on].agg(['count', 'mean'])\n",
    "    counts = agg['count']\n",
    "    means = agg['mean']\n",
    "\n",
    "    # Compute the \"smoothed\" means\n",
    "    smooth = (counts * means + m * mean) / (counts + m)\n",
    "\n",
    "    # Replace each value by the according smoothed mean\n",
    "    return df[by].map(smooth)\n",
    "class TargetEncoderCV(TargetEncoder):\n",
    "    \"\"\"Cross-fold target encoder.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, n_splits=3, shuffle=True, cols=None):\n",
    "        \"\"\"Cross-fold target encoding for categorical features.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        n_splits : int\n",
    "            Number of cross-fold splits. Default = 3.\n",
    "        shuffle : bool\n",
    "            Whether to shuffle the data when splitting into folds.\n",
    "        cols : list of str\n",
    "            Columns to target encode.\n",
    "        \"\"\"\n",
    "        self.n_splits = n_splits\n",
    "        self.shuffle = shuffle\n",
    "        self.cols = cols\n",
    "        \n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"Fit cross-fold target encoder to X and y\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        X : pandas DataFrame, shape [n_samples, n_columns]\n",
    "            DataFrame containing columns to encode\n",
    "        y : pandas Series, shape = [n_samples]\n",
    "            Target values.\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        self : encoder\n",
    "            Returns self.\n",
    "        \"\"\"\n",
    "        self._target_encoder = TargetEncoder(cols=self.cols)\n",
    "        self._target_encoder.fit(X, y)\n",
    "        return self\n",
    "\n",
    "    \n",
    "    def transform(self, X, y=None):\n",
    "        \"\"\"Perform the target encoding transformation.\n",
    "\n",
    "        Uses cross-fold target encoding for the training fold,\n",
    "        and uses normal target encoding for the test fold.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : pandas DataFrame, shape [n_samples, n_columns]\n",
    "            DataFrame containing columns to encode\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        pandas DataFrame\n",
    "            Input DataFrame with transformed columns\n",
    "        \"\"\"\n",
    "\n",
    "        # Use target encoding from fit() if this is test data\n",
    "        if y is None:\n",
    "            return self._target_encoder.transform(X)\n",
    "\n",
    "        # Compute means for each fold\n",
    "        self._train_ix = []\n",
    "        self._test_ix = []\n",
    "        self._fit_tes = []\n",
    "        kf = KFold(n_splits=self.n_splits, shuffle=self.shuffle)\n",
    "        for train_ix, test_ix in kf.split(X):\n",
    "            self._train_ix.append(train_ix)\n",
    "            self._test_ix.append(test_ix)\n",
    "            te = TargetEncoder(cols=self.cols)\n",
    "            if isinstance(X, pd.DataFrame):\n",
    "                self._fit_tes.append(te.fit(X.iloc[train_ix,:],\n",
    "                                            y.iloc[train_ix]))\n",
    "            elif isinstance(X, np.ndarray):\n",
    "                self._fit_tes.append(te.fit(X[train_ix,:],\n",
    "                                            y[train_ix]))\n",
    "            else:\n",
    "                raise TypeError('X must be DataFrame or ndarray')\n",
    "\n",
    "        # Apply means across folds\n",
    "        Xo = X.copy()\n",
    "        for ix in range(len(self._test_ix)):\n",
    "            test_ix = self._test_ix[ix]\n",
    "            if isinstance(X, pd.DataFrame):\n",
    "                Xo.iloc[test_ix,:] = \\\n",
    "                    self._fit_tes[ix].transform(X.iloc[test_ix,:])\n",
    "            elif isinstance(X, np.ndarray):\n",
    "                Xo[test_ix,:] = \\\n",
    "                    self._fit_tes[ix].transform(X[test_ix,:])\n",
    "            else:\n",
    "                raise TypeError('X must be DataFrame or ndarray')\n",
    "        return Xo\n",
    "\n",
    "            \n",
    "    def fit_transform(self, X, y=None):\n",
    "        \"\"\"Fit and transform the data via target encoding.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        X : pandas DataFrame, shape [n_samples, n_columns]\n",
    "            DataFrame containing columns to encode\n",
    "        y : pandas Series, shape = [n_samples]\n",
    "            Target values (required!).\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        pandas DataFrame\n",
    "            Input DataFrame with transformed columns\n",
    "        \"\"\"\n",
    "        return self.fit(X, y).transform(X, y)\n",
    "    \n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.style.use(\"classic\")\n",
    "rcParams['figure.figsize'] = 14,5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-15T19:21:26.772268Z",
     "start_time": "2022-10-15T19:21:25.818716Z"
    }
   },
   "outputs": [],
   "source": [
    "df5=pd.read_csv('df5',low_memory=False)\n",
    "\n",
    "# rescaling\n",
    "\n",
    "\n",
    "# normalization\n",
    "mms_age=pp.MinMaxScaler()\n",
    "df5['age']=mms_age.fit_transform(df5[['age']].values)\n",
    "\n",
    "mms_vintage=pp.MinMaxScaler()\n",
    "df5['vintage']=mms_vintage.fit_transform(df5[['vintage']].values)\n",
    "\n",
    "# mms_annual_premium=pp.MinMaxScaler()\n",
    "\n",
    "# robust scaler\n",
    "rs_annual_premium= pp.RobustScaler()\n",
    "df5['annual_premium'] = rs_annual_premium.fit_transform(df5[['annual_premium']].values)\n",
    "\n",
    "# standardization\n",
    "# ss_annual_premium=pp.StandardScaler()\n",
    "\n",
    "\n",
    "# ONE HOT\n",
    "df5=pd.get_dummies(df5,prefix='vehicle_age',columns=['vehicle_age'])\n",
    "df5=pd.get_dummies(df5,prefix='gender',columns=['gender'])\n",
    "\n",
    "\n",
    "# TARGET\n",
    "target_encode_region_code=df5.groupby('region_code')['response'].mean()\n",
    "df5.loc[:,'region_code']=df5['region_code'].map(target_encode_region_code)\n",
    "\n",
    "# FREQUENCY\n",
    "fe_policy_sales_channel=df5.groupby('policy_sales_channel').size()/len(df5)\n",
    "df5.loc[:,'policy_sales_channel']=df5['policy_sales_channel'].map(fe_policy_sales_channel)\n",
    "\n",
    "\n",
    "X=df5.drop('response',axis=1)\n",
    "y=df5['response'].copy()\n",
    "\n",
    "x_train, x_validation, y_train, y_validation = ms.train_test_split(X,y,test_size=0.20,stratify=y,random_state=42)\n",
    "\n",
    "df5=pd.concat ([x_train, y_train], axis=1) \n",
    "#################################################################################################\n",
    "# rescaling\n",
    "\n",
    "# normalization\n",
    "# age\n",
    "# x_validation.loc[:,'age']=mms_age.transform(x_validation[['age']].values)\n",
    "# # vintage\n",
    "# x_validation.loc[:,'vintage']=mms_vintage.transform(x_validation[['vintage']].values)\n",
    "\n",
    "\n",
    "# # robust scaler\n",
    "# # annual_premium\n",
    "# x_validation.loc[:,'annual_premium']=rs_annual_premium.transform(x_validation[['annual_premium']].values)\n",
    "\n",
    "\n",
    "# # standardization\n",
    "# # x_validation.loc[:,'annual_premium']=ss_annual_premium.transform(x_validation[['annual_premium']].values)\n",
    "\n",
    "# # ONE HOT\n",
    "# # gender\n",
    "# x_validation=pd.get_dummies(x_validation,prefix='gender',columns=['gender'])\n",
    "# # vehicle_age\n",
    "# x_validation=pd.get_dummies(x_validation,prefix='vehicle_age',columns=['vehicle_age'])\n",
    "\n",
    "\n",
    "# # TARGET\n",
    "# # region_code\n",
    "# x_validation.loc[:,'region_code']=x_validation.loc[:,'region_code'].map(target_encode_region_code) \n",
    "# # x_validation.loc[:,'gender']=x_validation.loc[:,'gender'].map(target_encode_gender)\n",
    "\n",
    "\n",
    "# # FREQUENCY\n",
    "# # policy_sales_channel\n",
    "# x_validation.loc[:,'policy_sales_channel']=x_validation['policy_sales_channel'].map(fe_policy_sales_channel)\n",
    "\n",
    "\n",
    "# fillna\n",
    "# x_validation=x_validation.fillna(0) # tem regiões (region_code) no teste q n estão no treino"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-15T19:19:44.336650Z",
     "start_time": "2022-10-15T19:19:44.331994Z"
    },
    "cell_style": "split",
    "code_folding": [
     0
    ],
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# feature selection\n",
    "# # model definition\n",
    "# forest=en.ExtraTreesClassifier(n_estimators=250,random_state=0,n_jobs=-1) \n",
    "\n",
    "# # recebe os dados de treino e a variável resposta\n",
    "# # data preparation\n",
    "# x_train_n=df5.drop(['id','response'],axis=1)\n",
    "# y_train_n=y_train.values\n",
    "# forest.fit(x_train_n,y_train_n)\n",
    "\n",
    "# importances=forest.feature_importances_\n",
    "# std=np.std([tree.feature_importances_ for tree in forest.estimators_],axis=0)\n",
    "# indices=np.argsort(importances)[::-1]  # índice é a importância das árvores ornada pelo maior valor\n",
    "\n",
    "# # print the feature ranking\n",
    "# print('Feature ranking')\n",
    "# df=pd.DataFrame()\n",
    "# for i, j in zip(x_train_n,forest.feature_importances_): # zip é pra arrumar as colunas com os valores\n",
    "#     aux=pd.DataFrame({'feature':i,'importance':j},index=[0])\n",
    "#     df=pd.concat([df,aux],axis=0)\n",
    "    \n",
    "# print(df.sort_values('importance',ascending=False))\n",
    "\n",
    "\n",
    "\n",
    "# df1=df.sort_values(by='importance',ascending=False)\n",
    "# df1=df1.drop('importance',axis=1)\n",
    "# df1['feature'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-15T19:21:33.232398Z",
     "start_time": "2022-10-15T19:21:33.210237Z"
    }
   },
   "outputs": [],
   "source": [
    "# cols_selected=df1['feature'].tolist()\n",
    "cols_selected=['vintage',\n",
    " 'annual_premium',\n",
    " 'age',\n",
    " 'region_code',\n",
    " 'vehicle_damage',\n",
    " 'policy_sales_channel',\n",
    " 'previously_insured',\n",
    " 'vehicle_age_below_1_year',\n",
    " 'vehicle_age_between_1_2_years',\n",
    " 'vehicle_age_over_2_years',\n",
    " 'gender_0',\n",
    " 'gender_1',\n",
    " 'driving_license']\n",
    "\n",
    "# cols_selected=['previously_insured', 'annual_premium', 'vintage', 'age',\n",
    "#        'region_code', 'policy_sales_channel', 'driving_license',\n",
    "#        'vehicle_damage','gender_Female', 'gender_Male',\n",
    "#        'vehicle_age_below_1_year', 'vehicle_age_between_1_2_years',\n",
    "#        'vehicle_age_over_2_years']\n",
    "\n",
    "x_train=df5[cols_selected]\n",
    "x_val=x_validation[cols_selected]\n",
    "y_val=y_validation.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# # accumulative gain\n",
    "# skplt.metrics.plot_cumulative_gain(y_val,yhat_knn);\n",
    "\n",
    "# df8=x_validation.copy()\n",
    "# df8['response']=y_validation.copy()       #response = propensity score\n",
    "\n",
    "# # propensity score\n",
    "# df8['score']=yhat_knn[:,1].tolist()\n",
    "\n",
    "# # sorte clients by propensity score\n",
    "# df8=df8.sort_values('score',ascending=False)\n",
    "\n",
    "# # reset index\n",
    "# df8=df8.reset_index(drop=True)\n",
    "\n",
    "# # create ranking order\n",
    "# df8['ranking']=df8.index+1    #(para ñ começar do zero)\n",
    "\n",
    "# df8['precision_at_k']=df8['response'].cumsum()/df8['ranking']\n",
    "\n",
    "# df8[['id','response','score','ranking','precision_at_k']].head(10)\n",
    "\n",
    "# precision_at_50=precision_at_k(df8,k=50)\n",
    "# print(precision_at_50)\n",
    "\n",
    "\n",
    "# recall_at_50=recall_at_k(df8,k=50)\n",
    "# print(recall_at_50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-15T19:33:22.772559Z",
     "start_time": "2022-10-15T19:32:46.262736Z"
    },
    "cell_style": "split",
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 86.03%\n"
     ]
    }
   ],
   "source": [
    "# model definition\n",
    "knn_model=nh.KNeighborsClassifier(n_neighbors=7)     \n",
    "\n",
    "# model training\n",
    "knn_model.fit(x_train,y_train)      \n",
    "\n",
    "# # model prediction                 \n",
    "# yhat_knn=knn_model.predict_proba(x_val)\n",
    "yhat_knn=knn_model.predict(x_val)\n",
    "\n",
    "print(\"Accuracy: %.2f%%\" % (accuracy_score(y_val, yhat_knn)*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-15T19:31:38.780890Z",
     "start_time": "2022-10-15T19:31:32.850814Z"
    },
    "cell_style": "split",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 87.74%\n"
     ]
    }
   ],
   "source": [
    "# model definition\n",
    "lr_model=lm.LogisticRegression(random_state=42)\n",
    "\n",
    "# model training\n",
    "lr_model.fit(x_train,y_train)\n",
    "\n",
    "# model prediction\n",
    "# yhat_lr=lr_model.predict_proba(x_val);\n",
    "yhat_lr=lr_model.predict(x_val);\n",
    "\n",
    "print(\"Accuracy: %.2f%%\" % (accuracy_score(y_val, yhat_lr)*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2022-09-29T19:01:59.643Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# # model definition\n",
    "# et=en.ExtraTreesClassifier(n_estimators=1000,n_jobs=-1,random_state=42)    # njobs-1 é pra usar os cores da minha máquina\n",
    "\n",
    "# # model training\n",
    "# et.fit(x_train,y_train)\n",
    "\n",
    "# # model predict\n",
    "# yhat_et=et.predict_proba(x_val)\n",
    "\n",
    "# # accumulative gain\n",
    "# skplt.metrics.plot_cumulative_gain(y_val,yhat_et);\n",
    "\n",
    "# df8=x_validation.copy()\n",
    "# df8['response']=y_validation.copy()       #response = propensity score\n",
    "\n",
    "# # propensity score\n",
    "# df8['score']=yhat_et[:,1].tolist()\n",
    "\n",
    "# # sorte clients by propensity score\n",
    "# df8=df8.sort_values('score',ascending=False)\n",
    "\n",
    "# # reset index\n",
    "# df8=df8.reset_index(drop=True)\n",
    "\n",
    "# # create ranking order\n",
    "# df8['ranking']=df8.index+1    #(para ñ começar do zero)\n",
    "\n",
    "# df8['precision_at_k']=df8['response'].cumsum()/df8['ranking']\n",
    "\n",
    "# df8[['id','response','score','ranking','precision_at_k']].head(10)\n",
    "\n",
    "# precision_at_50=precision_at_k(df8,k=50)\n",
    "# print(precision_at_50)\n",
    "\n",
    "\n",
    "# recall_at_50=recall_at_k(df8,k=50)\n",
    "# print(recall_at_50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-15T19:33:54.010339Z",
     "start_time": "2022-10-15T19:33:23.112290Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 87.73%\n"
     ]
    }
   ],
   "source": [
    "xgb_model = xgb.XGBClassifier(random_state=42)\n",
    "\n",
    "# model training\n",
    "xgb_model.fit(x_train,y_train)\n",
    "\n",
    "# model prediction\n",
    "# yhat_xgb=xgb_model.predict_proba(x_val);\n",
    "yhat_xgb=xgb_model.predict(x_val);\n",
    "\n",
    "print(\"Accuracy: %.2f%%\" % (accuracy_score(y_val, yhat_xgb)*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
